# -*- coding: utf-8 -*-
"""Big_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ry13YAKcBfSIw3--7HtCLNh6LpQJpF-B
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, count

# Inicializa SparkSession
spark = SparkSession.builder.appName("Conteo de Palabras en CSV").getOrCreate()

# Crea un DataFrame con los datos proporcionados
data = [
    ("COLORADO PEREZ MATEO ANDRES", "22"),
    ("DUEÑAS MARTINEZ ANDERSON RUGERY", "21"),
    ("HERNANDEZ GARCIA ANA MILENA", "17"),
    ("HERNANDEZ ROJAS SANTIAGO", "18"),
    ("HERNANDEZ MAHECHA OSCAR ALBERTO", "19"),
    ("HOLGUIN PINZON KEVIN JAIR", "20"),
    ("INFANTE LEAL JEISSON DAVID", "18"),
    ("LARA CARDENAS YOJAN DANIEL", "22"),
    ("MAHECHA BENAVIDES JUAN SEBASTIAN", "21"),
    ("MARTINEZ PULIDO JOHAN SEBASTIAN", "17"),
    ("MORENO LUGO JOHAN SEBASTIAN", "18"),
    ("PATERNINA ORTEGA DANIEL", "19"),
    ("PORRAS GARZON ANDRES FELIPE", "20"),
    ("REMOLINA ALVAREZ SAMUEL", "18"),
    ("ROJAS HERRERA JHONNY SEBASTIAN", "22"),
    ("ROJAS RODRIGUEZ DANIEL ALEXIS", "21"),
    ("SALDAÃ‘A CASTRO MIGUEL ALEJANDRO", "17")
]

columns = ["Nombre", "Edad"]

df = spark.createDataFrame(data, columns)

# Concatena las columnas en una sola
df = df.withColumn("texto", df["Nombre"] + " " + df["Edad"].cast("string"))

# Divide el contenido en palabras
words = df.select(explode(split(df["Nombre"], " ")).alias("palabra"))

# Realiza el conteo de palabras
word_counts = words.groupBy("palabra").count()

# Muestra el resultado
word_counts.show()

# Detén SparkSession
spark.stop()

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

# Configuración de Spark
conf = SparkConf().setAppName("Conteo de Palabras en CSV")
sc = SparkContext(conf=conf)
spark = SparkSession(sc)

# Lee el archivo CSV como un DataFrame
data = spark.read.csv("/content/Datosf.csv", header=True, inferSchema=True)

# Convierte el DataFrame a un RDD de filas (Row)
rdd = data.rdd

# Divide cada fila en palabras y cuenta las palabras
word_count = rdd.flatMap(lambda row: row[0].split()).count()

print("Número de palabras en el archivo CSV:", word_count)

# Detén el contexto Spark
sc.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

# Inicializa SparkSession
spark = SparkSession.builder.appName("PromedioEdad").getOrCreate()

# Carga un archivo CSV con una columna llamada "edad"
df = spark.read.csv("/content/Datosf.csv", header=True, inferSchema=True)

# Calcula el promedio de edad
avg_age = df.select(avg("edad")).first()[0]

# Imprime el resultado
print("Promedio de edad:", avg_age)

# Detén SparkSession
spark.stop()

from pyspark.sql import SparkSession

# Inicializa SparkSession
spark = SparkSession.builder.appName("PersonasMayores").getOrCreate()

# Carga un archivo CSV con una columna llamada "edad"
df = spark.read.csv("/content/Datosf.csv", header=True, inferSchema=True)

# Filtra las personas con edades mayores a 20
personas_mayores = df.filter(df.Edad> 20)

# Muestra el resultado
personas_mayores.show()

# Detén SparkSession
spark.stop()